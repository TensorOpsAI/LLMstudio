{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:56574 Running LLMstudio Tracking on http://localhost:56575 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = LLM(\"openai/gpt-3.5-turbo\")\n",
    "llm = LLM(\"vertexai/gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What's your name?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Can you sing a song?\",\n",
    "    \"Tell me about yourself.\",\n",
    "    \"What's your favorite color?\",\n",
    "    \"Do you have any pets?\",\n",
    "    \"What's your favorite food?\",\n",
    "    \"What's your favorite book?\",\n",
    "    \"Who is your favorite author?\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Who is your favorite actor?\",\n",
    "    \"What's your favorite song?\",\n",
    "    \"Who is your favorite singer?\",\n",
    "    \"What's your favorite sport?\",\n",
    "    \"Who is your favorite athlete?\",\n",
    "    \"What's your favorite team?\",\n",
    "    \"What's your favorite city?\",\n",
    "    \"What's your favorite country?\",\n",
    "    \"What's your favorite place?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting chat responses:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting chat responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = llm.batch_chat(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished requests: 1/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 2/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 3/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 4/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 0\n",
      "Finished requests: 5/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 1\n",
      "Finished requests: 6/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 1\n",
      "Finished requests: 7/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 8/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 9/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 10/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 11/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 12/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 13/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 14/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 15/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 16/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 17/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 18/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 19/20\n",
      "Amount of parallel requests being allowed: 20\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n",
      "Finished requests: 20/20\n",
      "Amount of parallel requests being allowed: 25\n",
      "Max tokens being used: None\n",
      "Requests finished with an error: 2\n"
     ]
    }
   ],
   "source": [
    "responses = llm.batch_chat(inputs=inputs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Exception(\"'NoneType' object is not subscriptable\"),\n",
       " ChatCompletion(id='4e20220b-5892-4e16-95b4-1f77b75c9c9a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything! \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Tell me a joke.', chat_output=\"Why don't scientists trust atoms? \\n\\nBecause they make up everything! \\n\", context=[{'role': 'user', 'content': 'Tell me a joke.'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.254836, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 5, 'output_tokens': 15, 'total_tokens': 20, 'cost_usd': 1.75e-05, 'latency_s': 2.181427001953125, 'time_to_first_token_s': 2.1808950901031494, 'inter_token_latency_s': 7.281303405761718e-05, 'tokens_per_second': 2.7504931380366813}),\n",
       " ChatCompletion(id='055e9222-3ad8-4221-8df0-c83d03a19e43', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I do not have access to real-time information, including weather conditions. To get the current weather, I recommend checking a reliable weather app or website like:\\n\\n* **AccuWeather**\\n* **The Weather Channel**\\n* **National Weather Service** (for the USA)\\n* **Met Office** (for the UK)\\n\\nPlease provide me with your location so I can help you find the right resource. ðŸ˜Š \\n', role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's the weather like?\", chat_output='I do not have access to real-time information, including weather conditions. To get the current weather, I recommend checking a reliable weather app or website like:\\n\\n* **AccuWeather**\\n* **The Weather Channel**\\n* **National Weather Service** (for the USA)\\n* **Met Office** (for the UK)\\n\\nPlease provide me with your location so I can help you find the right resource. ðŸ˜Š \\n', context=[{'role': 'user', 'content': \"What's the weather like?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.044162, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 83, 'total_tokens': 89, 'cost_usd': 8.924999999999999e-05, 'latency_s': 1.9698891639709473, 'time_to_first_token_s': 1.9687190055847168, 'inter_token_latency_s': 0.00012087821960449219, 'tokens_per_second': 5.076427741671401}),\n",
       " ChatCompletion(id='86ece802-14b0-4271-896a-e90777f1db50', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I can't sing a song in the traditional sense. I am a language model, and I don't have a voice or the ability to produce sound. \\n\\nHowever, I can write you a song!  Tell me what kind of song you'd like, and I'll give it a try:\\n\\n* **What genre?** (Pop, rock, country, folk, etc.)\\n* **What's the mood?** (Happy, sad, romantic, funny, etc.)\\n* **Any specific themes?** (Love, friendship, nature, adventure, etc.)\\n\\nLet me know, and I'll do my best to create a song for you! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030754, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Can you sing a song?', chat_output=\"I can't sing a song in the traditional sense. I am a language model, and I don't have a voice or the ability to produce sound. \\n\\nHowever, I can write you a song!  Tell me what kind of song you'd like, and I'll give it a try:\\n\\n* **What genre?** (Pop, rock, country, folk, etc.)\\n* **What's the mood?** (Happy, sad, romantic, funny, etc.)\\n* **Any specific themes?** (Love, friendship, nature, adventure, etc.)\\n\\nLet me know, and I'll do my best to create a song for you! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Can you sing a song?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.025928, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 134, 'total_tokens': 140, 'cost_usd': 0.00014279999999999997, 'latency_s': 1.9510650634765625, 'time_to_first_token_s': 0.911005973815918, 'inter_token_latency_s': 0.09165672822432085, 'tokens_per_second': 6.15048684159074}),\n",
       " Exception(\"'NoneType' object is not subscriptable\"),\n",
       " ChatCompletion(id='0cec5960-5697-4a2c-a016-adec3346e614', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite colors. ðŸ˜Š  But I can tell you that blue is often associated with feelings of calmness, trust, and stability.  What's your favorite color? ðŸŽ¨ \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite color?\", chat_output=\"As an AI, I don't have personal preferences like favorite colors. ðŸ˜Š  But I can tell you that blue is often associated with feelings of calmness, trust, and stability.  What's your favorite color? ðŸŽ¨ \\n\", context=[{'role': 'user', 'content': \"What's your favorite color?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.062591, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 49, 'total_tokens': 55, 'cost_usd': 5.3549999999999994e-05, 'latency_s': 1.9848380088806152, 'time_to_first_token_s': 1.9842331409454346, 'inter_token_latency_s': 7.952962602887834e-05, 'tokens_per_second': 4.030555624290842}),\n",
       " ChatCompletion(id='83e870d2-df10-4040-9f1f-f551b03e7c80', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have any pets. I am a computer program, and I don't have a physical body. I can't experience the world in the same way that a person with pets does. However, I can access and process information about pets, and I can generate text about them. For example, I can tell you about the different types of pets, or I can write a story about a pet.  \\n\\nWhat about you? Do you have any pets? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Do you have any pets?', chat_output=\"As a large language model, I don't have any pets. I am a computer program, and I don't have a physical body. I can't experience the world in the same way that a person with pets does. However, I can access and process information about pets, and I can generate text about them. For example, I can tell you about the different types of pets, or I can write a story about a pet.  \\n\\nWhat about you? Do you have any pets? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Do you have any pets?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.037476, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 102, 'total_tokens': 108, 'cost_usd': 0.00010919999999999998, 'latency_s': 1.9550681114196777, 'time_to_first_token_s': 1.952833890914917, 'inter_token_latency_s': 0.00018856742165305397, 'tokens_per_second': 6.137893575117529}),\n",
       " ChatCompletion(id='d58dca23-41f0-48fc-b83e-4480482ec2df', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have personal preferences like favorite foods. I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nFor example, I can tell you that pizza is a very popular food, or that sushi is a delicious Japanese dish. But I can't tell you what *my* favorite food is, because I don't have one! \\n\\nWhat's your favorite food? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite food?\", chat_output=\"As a large language model, I don't have personal preferences like favorite foods. I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nFor example, I can tell you that pizza is a very popular food, or that sushi is a delicious Japanese dish. But I can't tell you what *my* favorite food is, because I don't have one! \\n\\nWhat's your favorite food? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite food?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.051688, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 94, 'total_tokens': 100, 'cost_usd': 0.00010079999999999998, 'latency_s': 1.9722049236297607, 'time_to_first_token_s': 1.9717977046966553, 'inter_token_latency_s': 2.965060147372159e-05, 'tokens_per_second': 6.0845604106466284}),\n",
       " ChatCompletion(id='60e64ab7-e973-4786-b14a-9b421ba24b99', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have personal preferences like favorite books. I can access and process information from a massive dataset of text and code, but I don't have feelings or emotions. \\n\\nHowever, I can help you find a book you might enjoy! What kind of books do you like? Tell me about your favorite genres, authors, or themes, and I can give you some recommendations. ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite book?\", chat_output=\"As a large language model, I don't have personal preferences like favorite books. I can access and process information from a massive dataset of text and code, but I don't have feelings or emotions. \\n\\nHowever, I can help you find a book you might enjoy! What kind of books do you like? Tell me about your favorite genres, authors, or themes, and I can give you some recommendations. ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite book?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.259567, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 85, 'total_tokens': 91, 'cost_usd': 9.134999999999998e-05, 'latency_s': 2.1808922290802, 'time_to_first_token_s': 2.1803550720214844, 'inter_token_latency_s': 5.367067125108507e-05, 'tokens_per_second': 4.585279302965621}),\n",
       " ChatCompletion(id='61943f28-1abf-41ac-a429-f15ba236575f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite authors. I can access and process information from a vast database, but I don't experience emotions or form personal opinions. \\n\\nHowever, I can tell you about some authors who are widely acclaimed and whose works are often studied and discussed:\\n\\n* **William Shakespeare:** A renowned playwright and poet, often considered the greatest writer in the English language.\\n* **Jane Austen:** A celebrated novelist known for her witty and insightful social commentary.\\n* **Charles Dickens:** A master of Victorian literature, famous for his social realism and memorable characters.\\n* **Leo Tolstoy:** A Russian novelist and philosopher known for his epic novels, such as *War and Peace*.\\n* **Virginia Woolf:** A modernist writer known for her stream-of-consciousness style and exploration of the female experience.\\n\\nThese are just a few examples. There are many other talented authors out there! What kind of books do you enjoy? I can help you find new authors and books based on your interests. \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite author?', chat_output=\"As an AI, I don't have personal preferences like favorite authors. I can access and process information from a vast database, but I don't experience emotions or form personal opinions. \\n\\nHowever, I can tell you about some authors who are widely acclaimed and whose works are often studied and discussed:\\n\\n* **William Shakespeare:** A renowned playwright and poet, often considered the greatest writer in the English language.\\n* **Jane Austen:** A celebrated novelist known for her witty and insightful social commentary.\\n* **Charles Dickens:** A master of Victorian literature, famous for his social realism and memorable characters.\\n* **Leo Tolstoy:** A Russian novelist and philosopher known for his epic novels, such as *War and Peace*.\\n* **Virginia Woolf:** A modernist writer known for her stream-of-consciousness style and exploration of the female experience.\\n\\nThese are just a few examples. There are many other talented authors out there! What kind of books do you enjoy? I can help you find new authors and books based on your interests. \\n\", context=[{'role': 'user', 'content': 'Who is your favorite author?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.248588, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 208, 'total_tokens': 214, 'cost_usd': 0.00022049999999999997, 'latency_s': 2.1665587425231934, 'time_to_first_token_s': 1.984015703201294, 'inter_token_latency_s': 0.010329945882161459, 'tokens_per_second': 7.384983239072604}),\n",
       " ChatCompletion(id='3e5f27e1-216d-464e-96e6-b33f3168eaa2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite movies. I can, however, access and process information about millions of movies! \\n\\nWhat kind of movies do you enjoy? Maybe I can suggest some based on your taste. ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030753, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite movie?\", chat_output=\"As an AI, I don't have personal preferences like favorite movies. I can, however, access and process information about millions of movies! \\n\\nWhat kind of movies do you enjoy? Maybe I can suggest some based on your taste. ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite movie?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030753.96323, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 50, 'total_tokens': 56, 'cost_usd': 5.459999999999999e-05, 'latency_s': 0.8847799301147461, 'time_to_first_token_s': 0.5166809558868408, 'inter_token_latency_s': 0.048533269337245395, 'tokens_per_second': 9.04179641480169}),\n",
       " ChatCompletion(id='3a8ef83c-b884-40a6-8834-24027e4dfc1a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite actors! I can't experience movies or performances in the way humans do. \\n\\nBut I can tell you about many talented actors! Who are some of your favorites? Maybe we can discuss their work. ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030755, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite actor?', chat_output=\"As an AI, I don't have personal preferences like favorite actors! I can't experience movies or performances in the way humans do. \\n\\nBut I can tell you about many talented actors! Who are some of your favorites? Maybe we can discuss their work. ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Who is your favorite actor?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030755.057879, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 55, 'total_tokens': 61, 'cost_usd': 5.984999999999999e-05, 'latency_s': 1.9745159149169922, 'time_to_first_token_s': 1.97369384765625, 'inter_token_latency_s': 6.972040448869978e-05, 'tokens_per_second': 4.051625990736224}),\n",
       " ChatCompletion(id='a3e961c5-9411-473e-9f7b-9fdee48c90b9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a language model, I don't have personal preferences like favorite songs. I can process information and generate responses, but I don't have emotions or experiences to form opinions. \\n\\nWhat about you? What's your favorite song? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030756, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite song?\", chat_output=\"As a language model, I don't have personal preferences like favorite songs. I can process information and generate responses, but I don't have emotions or experiences to form opinions. \\n\\nWhat about you? What's your favorite song? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite song?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030756.643143, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 49, 'total_tokens': 55, 'cost_usd': 5.3549999999999994e-05, 'latency_s': 1.3774540424346924, 'time_to_first_token_s': 1.2855329513549805, 'inter_token_latency_s': 0.009314571108136858, 'tokens_per_second': 5.807816270849773}),\n",
       " ChatCompletion(id='02ecdead-aba3-4e85-91c2-5f779471913e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences or favorites like humans do. I don't listen to music or have the capacity to develop opinions about singers. \\n\\nWhat about you? Who is your favorite singer? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030757, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite singer?', chat_output=\"As an AI, I don't have personal preferences or favorites like humans do. I don't listen to music or have the capacity to develop opinions about singers. \\n\\nWhat about you? Who is your favorite singer? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Who is your favorite singer?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030757.571306, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 46, 'total_tokens': 52, 'cost_usd': 5.039999999999999e-05, 'latency_s': 2.3055031299591064, 'time_to_first_token_s': 2.3043882846832275, 'inter_token_latency_s': 0.00014625276838030134, 'tokens_per_second': 3.469958420807652}),\n",
       " ChatCompletion(id='63c0615b-fdcd-48fa-b35d-d34a8ef2e8f9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a large language model, I don't have personal preferences or feelings. I don't have a favorite sport! \\n\\nHowever, I can tell you about many different sports and their rules, history, and famous players.  What kind of sports are you interested in? âš½ï¸ðŸ€ðŸˆâš¾ï¸ \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030756, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite sport?\", chat_output=\"As a large language model, I don't have personal preferences or feelings. I don't have a favorite sport! \\n\\nHowever, I can tell you about many different sports and their rules, history, and famous players.  What kind of sports are you interested in? âš½ï¸ðŸ€ðŸˆâš¾ï¸ \\n\", context=[{'role': 'user', 'content': \"What's your favorite sport?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030756.2730932, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 69, 'total_tokens': 75, 'cost_usd': 7.455e-05, 'latency_s': 1.0068690776824951, 'time_to_first_token_s': 0.4799478054046631, 'inter_token_latency_s': 0.054658439424302846, 'tokens_per_second': 9.931777846447469}),\n",
       " ChatCompletion(id='a4d35c37-9645-4e54-a29c-18f6e8cda863', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite athletes. I can access and process information, but I don't have feelings or opinions. \\n\\nCan I help you learn more about a specific athlete, or find information about sports in general? ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030756, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Who is your favorite athlete?', chat_output=\"As an AI, I don't have personal preferences like favorite athletes. I can access and process information, but I don't have feelings or opinions. \\n\\nCan I help you learn more about a specific athlete, or find information about sports in general? ðŸ˜Š \\n\", context=[{'role': 'user', 'content': 'Who is your favorite athlete?'}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030756.649524, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 53, 'total_tokens': 59, 'cost_usd': 5.7749999999999994e-05, 'latency_s': 1.3825159072875977, 'time_to_first_token_s': 1.3820490837097168, 'inter_token_latency_s': 5.3882598876953125e-05, 'tokens_per_second': 5.786551863765139}),\n",
       " ChatCompletion(id='4e8bc4bd-d723-4b04-a9b6-31bc9704218e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite teams. I'm here to help you with any questions or tasks you might have! ðŸ˜Š\\n\\nIs there anything else I can help you with today? \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030757, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite team?\", chat_output=\"As an AI, I don't have personal preferences like favorite teams. I'm here to help you with any questions or tasks you might have! ðŸ˜Š\\n\\nIs there anything else I can help you with today? \\n\", context=[{'role': 'user', 'content': \"What's your favorite team?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030757.5779, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 44, 'total_tokens': 50, 'cost_usd': 4.8299999999999995e-05, 'latency_s': 2.3103432655334473, 'time_to_first_token_s': 2.3098092079162598, 'inter_token_latency_s': 6.273814610072545e-05, 'tokens_per_second': 3.4626889083310477}),\n",
       " ChatCompletion(id='55baf702-e637-4442-aed1-ef2165b5c555', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As an AI, I don\\'t have personal preferences or the ability to experience cities like humans do. I can\\'t have a \"favorite\" city! \\n\\nHowever, I can tell you about many interesting cities around the world.  What kind of city are you interested in? Do you want a bustling metropolis, a charming historic town, a beautiful coastal city, or something else? \\n\\nTell me more about your interests, and I can help you learn about some amazing places! ðŸ˜Š \\n', role='assistant', function_call=None, tool_calls=None))], created=1725030757, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite city?\", chat_output='As an AI, I don\\'t have personal preferences or the ability to experience cities like humans do. I can\\'t have a \"favorite\" city! \\n\\nHowever, I can tell you about many interesting cities around the world.  What kind of city are you interested in? Do you want a bustling metropolis, a charming historic town, a beautiful coastal city, or something else? \\n\\nTell me more about your interests, and I can help you learn about some amazing places! ðŸ˜Š \\n', context=[{'role': 'user', 'content': \"What's your favorite city?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030757.584664, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 99, 'total_tokens': 105, 'cost_usd': 0.00010604999999999999, 'latency_s': 2.3169500827789307, 'time_to_first_token_s': 2.3165531158447266, 'inter_token_latency_s': 2.746148542924361e-05, 'tokens_per_second': 5.179222499954466}),\n",
       " ChatCompletion(id='9b72808e-6bec-4977-903a-1246858648f0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI, I don't have personal preferences like favorite countries. I don't experience the world in the same way humans do! \\n\\nWhat about you?  Do you have a favorite country?  I'd love to hear about it! ðŸ˜Š \\n\", role='assistant', function_call=None, tool_calls=None))], created=1725030756, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite country?\", chat_output=\"As an AI, I don't have personal preferences like favorite countries. I don't experience the world in the same way humans do! \\n\\nWhat about you?  Do you have a favorite country?  I'd love to hear about it! ðŸ˜Š \\n\", context=[{'role': 'user', 'content': \"What's your favorite country?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030756.545996, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 52, 'total_tokens': 58, 'cost_usd': 5.669999999999999e-05, 'latency_s': 1.2783191204071045, 'time_to_first_token_s': 1.0167860984802246, 'inter_token_latency_s': 0.03341742924281529, 'tokens_per_second': 6.25821821193776}),\n",
       " ChatCompletion(id='4943fe64-3270-4cbf-8126-0cf1f3dc3988', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As a large language model, I don\\'t have personal preferences like \"favorite places\".  I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nHowever, I can tell you about some popular and beautiful places in the world based on what people search for:\\n\\n* **The Grand Canyon:** This natural wonder in Arizona is known for its immense size and breathtaking views.\\n* **The Great Barrier Reef:** This coral reef system off the coast of Australia is home to an incredible diversity of marine life.\\n* **Machu Picchu:** This ancient Inca citadel in Peru is a testament to the ingenuity and beauty of this lost civilization.\\n* **The Northern Lights:** This celestial phenomenon, visible in the northern hemisphere, is a spectacular display of light and color.\\n* **The Eiffel Tower:** This iconic structure in Paris is a symbol of romance and elegance.\\n\\nWhat are some of your favorite places in the world? I\\'d love to hear about them! \\n', role='assistant', function_call=None, tool_calls=None))], created=1725030757, model='gemini-1.5-flash', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input=\"What's your favorite place?\", chat_output='As a large language model, I don\\'t have personal preferences like \"favorite places\".  I can access and process information from the real world through Google Search and keep my response consistent with search results. \\n\\nHowever, I can tell you about some popular and beautiful places in the world based on what people search for:\\n\\n* **The Grand Canyon:** This natural wonder in Arizona is known for its immense size and breathtaking views.\\n* **The Great Barrier Reef:** This coral reef system off the coast of Australia is home to an incredible diversity of marine life.\\n* **Machu Picchu:** This ancient Inca citadel in Peru is a testament to the ingenuity and beauty of this lost civilization.\\n* **The Northern Lights:** This celestial phenomenon, visible in the northern hemisphere, is a spectacular display of light and color.\\n* **The Eiffel Tower:** This iconic structure in Paris is a symbol of romance and elegance.\\n\\nWhat are some of your favorite places in the world? I\\'d love to hear about them! \\n', context=[{'role': 'user', 'content': \"What's your favorite place?\"}], provider='vertexai', deployment='gemini-1.5-flash', timestamp=1725030757.559865, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 6, 'output_tokens': 205, 'total_tokens': 211, 'cost_usd': 0.00021734999999999997, 'latency_s': 2.29160213470459, 'time_to_first_token_s': 1.3863952159881592, 'inter_token_latency_s': 0.058922735850016277, 'tokens_per_second': 6.982014791176898})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear chat calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "sync_responses = []\n",
    "from tqdm import tqdm\n",
    "for input in tqdm(inputs):\n",
    "    sync_responses = await llm.async_chat(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
