{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 Running LLMstudio Tracking on http://localhost:50002 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM\n",
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "import requests\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Vertex Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemini-1.5-flash'\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent?alt=sse\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-goog-api-key\": api_key,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = [\n",
    "      {'name': 'multiply',\n",
    "       'description': 'Returns the product of two numbers.',\n",
    "       'parameters': {'type_': 'OBJECT',\n",
    "       'properties': {\n",
    "         'a': {'type_': 'NUMBER'},\n",
    "         'b': {'type_': 'NUMBER'} },\n",
    "       'required': ['a', 'b']} }]\n",
    "\n",
    "calculator_2 = \"\"\"function_declarations {\n",
    "  name: \"multiply\"\n",
    "  description: \"Returns the product of two numbers.\"\n",
    "  parameters {\n",
    "    type_: OBJECT\n",
    "    properties {\n",
    "      key: \"b\"\n",
    "      value {\n",
    "        type_: NUMBER\n",
    "      }\n",
    "    }\n",
    "    properties {\n",
    "      key: \"a\"\n",
    "      value {\n",
    "        type_: NUMBER\n",
    "      }\n",
    "    }\n",
    "    required: \"a\"\n",
    "    required: \"b\"\n",
    "  }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'What is 9 times 10?'\n",
    "data = {\n",
    "  \"system_instruction\": {\n",
    "    \"parts\": {\n",
    "      \"text\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"contents\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "        {\n",
    "          \"text\": \"How many legs does a spider have?\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, headers=headers, json=data, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data: {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \"A\"}],\"role\": \"model\"},\"finishReason\": \"STOP\",\"index\": 0}],\"usageMetadata\": {\"promptTokenCount\": 9,\"candidatesTokenCount\": 1,\"totalTokenCount\": 10}}\\r\\n\\r\\ndata: {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \" spider has **eight** legs. \\\\n\"}],\"role\": \"model\"},\"finishReason\": \"STOP\",\"index\": 0,\"safetyRatings\": [{\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_HATE_SPEECH\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_HARASSMENT\",\"probability\": \"NEGLIGIBLE\"},{\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\"probability\": \"NEGLIGIBLE\"}]}],\"usageMetadata\": {\"promptTokenCount\": 9,\"candidatesTokenCount\": 8,\"totalTokenCount\": 17}}\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vertex Functions Tests - Str to OAI to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"system_instruction\": {\n",
      "    \"parts\": {\n",
      "      \"text\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"How many legs does a spider have?\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"system_instruction\": {\n",
      "    \"parts\": {\n",
      "      \"text\": \"You are a helpful assistant.\"\n",
      "    }\n",
      "  },\n",
      "  \"contents\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"Hello\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"model\",\n",
      "      \"parts\": [\n",
      "        {\n",
      "          \"text\": \"Hi there!\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Error: Input must be a list of dictionaries, each containing 'role' and 'content' keys.\n"
     ]
    }
   ],
   "source": [
    "def convert_openai_to_vertexai(input_data):\n",
    "    # Check if the input is a simple string\n",
    "    if isinstance(input_data, str):\n",
    "        # Return a Vertex AI formatted message with a user message\n",
    "        return {\n",
    "            \"system_instruction\": {\n",
    "                \"parts\": {\n",
    "                    \"text\": \"\"  # Empty system instruction\n",
    "                }\n",
    "            },\n",
    "            \"contents\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [{\"text\": input_data}]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Validate if input_data is a list and each element is a dictionary with the correct structure\n",
    "    if not isinstance(input_data, list) or not all(isinstance(msg, dict) and 'role' in msg and 'content' in msg for msg in input_data):\n",
    "        raise ValueError(\"Input must be a list of dictionaries, each containing 'role' and 'content' keys.\")\n",
    "\n",
    "    # Initialize the Vertex AI format if the input is not a simple string\n",
    "    vertexai_format = {\n",
    "        \"system_instruction\": {\n",
    "            \"parts\": {\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "        },\n",
    "        \"contents\": []\n",
    "    }\n",
    "    \n",
    "    # Loop through the OpenAI formatted messages\n",
    "    for message in input_data:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            # Set the system instruction\n",
    "            vertexai_format[\"system_instruction\"][\"parts\"][\"text\"] = message[\"content\"]\n",
    "        elif message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "            # Convert roles: 'assistant' -> 'model'\n",
    "            role = \"model\" if message[\"role\"] == \"assistant\" else \"user\"\n",
    "            # Append the message to the contents list in Vertex AI format\n",
    "            vertexai_format[\"contents\"].append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": message[\"content\"]}]\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {message['role']}. Expected 'system', 'user', or 'assistant'.\")\n",
    "    \n",
    "    return vertexai_format\n",
    "\n",
    "# Example usage with a string input\n",
    "simple_input = \"How many legs does a spider have?\"\n",
    "vertexai_format = convert_openai_to_vertexai(simple_input)\n",
    "print(json.dumps(vertexai_format, indent=2))\n",
    "\n",
    "# Example usage with a valid list of OpenAI messages\n",
    "openai_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n",
    "]\n",
    "vertexai_format = convert_openai_to_vertexai(openai_messages)\n",
    "print(json.dumps(vertexai_format, indent=2))\n",
    "\n",
    "# Example usage with an invalid input to trigger an error\n",
    "try:\n",
    "    invalid_input = [\n",
    "        {\"role\": \"system\", \"text\": \"You are a helpful assistant.\"}  # 'text' should be 'content'\n",
    "    ]\n",
    "    vertexai_format = convert_openai_to_vertexai(invalid_input)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vertex Chat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 Running LLMstudio Tracking on http://localhost:50002 \n",
      "\n",
      "tools: None\n",
      "message: {'system_instruction': {'parts': {'text': 'You are a helpful assistant'}}, 'contents': [{'role': 'user', 'parts': [{'text': 'Hello'}]}], 'tools': None, 'tool_config': {'function_calling_config': {'mode': 'AUTO'}}}\n",
      "chunk: {'parts': [{'text': 'Hello'}], 'role': 'model'}\n",
      "chunk: {'parts': [{'text': '! ðŸ‘‹  What can I do for you today? ðŸ˜Š \\n'}], 'role': 'model'}\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('newvertex/gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 String format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='0b661363-776e-48a3-a65e-6e0d08306ec8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! ðŸ‘‹  What can I do for you today? ðŸ˜Š \\n', role='assistant', function_call=None, tool_calls=None))], created=1723800713, model='gemini-1.5-pro-latest', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello', chat_output='Hello! ðŸ‘‹  What can I do for you today? ðŸ˜Š \\n', context=[{'role': 'user', 'content': 'Hello'}], provider='newvertex', deployment='gemini-1.5-pro-latest', timestamp=1723800713.506577, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 1, 'output_tokens': 16, 'total_tokens': 17, 'cost_usd': 1.7149999999999997e-05, 'latency_s': 1.8147468566894531, 'time_to_first_token_s': 1.448904037475586, 'inter_token_latency_s': 0.11205267906188965, 'tokens_per_second': 2.204164170476641})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OpenAI format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='7fa9938b-1ec8-4b7a-a89b-34c0bb779dd0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Aright man, mi deh yah a gwaan! How tings gwaan wid you?  ðŸ‡¯ðŸ‡² ðŸ˜„ \\n\\n(Translation: I'm doing well, how are you?) \\n\", role='assistant', function_call=None, tool_calls=None))], created=1723799492, model='gemini-1.5-pro-latest', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='Hello, how are you?', chat_output=\"Aright man, mi deh yah a gwaan! How tings gwaan wid you?  ðŸ‡¯ðŸ‡² ðŸ˜„ \\n\\n(Translation: I'm doing well, how are you?) \\n\", context=[{'role': 'system', 'content': 'You are Jamaican.'}, {'role': 'assistant', 'content': 'How can i help you?'}, {'role': 'user', 'content': 'Hello, how are you?'}], provider='newvertex', deployment='gemini-1.5-pro-latest', timestamp=1723799492.7148879, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 16, 'output_tokens': 46, 'total_tokens': 62, 'cost_usd': 5.3899999999999996e-05, 'latency_s': 2.1366829872131348, 'time_to_first_token_s': 1.3183910846710205, 'inter_token_latency_s': 0.11596315247671944, 'tokens_per_second': 3.7441211671902535})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You are Jamaican.\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"How can i help you?\"},\n",
    "           {\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "\n",
    "llm.chat(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vertex function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "  \"function_declarations\": [\n",
    "    {\n",
    "      \"name\": \"get_weather\",\n",
    "      \"description\": \"Gets the weather for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the weather for. For example: San Francisco\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"get_time\",\n",
    "      \"description\": \"Gets the current time for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the current time for. For example: New York\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"  \n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Engine on http://localhost:50001 "
     ]
    }
   ],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running LLMstudio Tracking on http://localhost:50002 \n",
      "tools: {'function_declarations': [{'name': 'get_weather', 'description': 'Gets the weather for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the weather for. For example: San Francisco'}}, 'required': ['location']}}, {'name': 'get_time', 'description': 'Gets the current time for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the current time for. For example: New York'}}, 'required': ['location']}}]}\n",
      "message: {'system_instruction': {'parts': {'text': 'You are a helpfull assistant'}}, 'contents': [{'role': 'model', 'parts': [{'text': 'How can i help you?'}]}, {'role': 'user', 'parts': [{'text': 'What time is it in San Francisco?'}]}], 'tools': {'function_declarations': [{'name': 'get_weather', 'description': 'Gets the weather for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the weather for. For example: San Francisco'}}, 'required': ['location']}}, {'name': 'get_time', 'description': 'Gets the current time for a certain location.', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location we are getting the current time for. For example: New York'}}, 'required': ['location']}}]}, 'tool_config': {'function_calling_config': {'mode': 'AUTO'}}}\n",
      "ChatCompletionChunk(id='bf50198b-e162-4f5e-824c-6645af623ee6', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_testid1234', function=ChoiceDeltaToolCallFunction(arguments='', name='get_time', type='function'), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723811876, model='gemini-1.5-pro-latest', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='d7f59eda-6e15-4064-9367-9a88a12f0c74', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=\"{'location': 'San Francisco'}\", name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723811876, model='gemini-1.5-pro-latest', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='fd8a2702-ec6e-4014-b4f9-b46d3748ba3f', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None)], created=1723811876, model='gemini-1.5-pro-latest', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "[{'id': 'chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': 'assistant', 'tool_calls': None}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723811876, 'model': 'gemini-1.5-pro-latest', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}, {'id': 'bf50198b-e162-4f5e-824c-6645af623ee6', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': 'assistant', 'tool_calls': [{'index': 0, 'id': 'call_testid1234', 'function': {'arguments': '', 'name': 'get_time', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723811876, 'model': 'gemini-1.5-pro-latest', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}, {'id': 'd7f59eda-6e15-4064-9367-9a88a12f0c74', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': \"{'location': 'San Francisco'}\", 'name': None}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723811876, 'model': 'gemini-1.5-pro-latest', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}, {'id': 'fd8a2702-ec6e-4014-b4f9-b46d3748ba3f', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'tool_calls', 'index': 0, 'logprobs': None}], 'created': 1723811876, 'model': 'gemini-1.5-pro-latest', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}]\n",
      "Tool calls: [{'index': 0, 'id': 'call_testid1234', 'function': {'arguments': '', 'name': 'get_time', 'type': 'function'}, 'type': None}, {'index': 0, 'id': None, 'function': {'arguments': \"{'location': 'San Francisco'}\", 'name': None}, 'type': None}]\n",
      "Tool call id: call_testid1234\n",
      "Tool call name: get_time\n",
      "Tool call type: function\n",
      "Tool call arguments: {'location': 'San Francisco'}\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('newvertex/gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='8968a00d-8d0b-431c-a8fb-c2545b43e97b', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_testid1234', function=Function(arguments=\"{'location': 'San Francisco'}\", name='get_time'), type='function')]))], created=1723811876, model='gemini-1.5-pro-latest', object='chat.completion', system_fingerprint=None, usage=None, session_id=None, chat_input='What time is it in San Francisco?', chat_output=\"{'location': 'San Francisco'}\", context=[{'role': 'system', 'content': 'You are a helpfull assistant'}, {'role': 'assistant', 'content': 'How can i help you?'}, {'role': 'user', 'content': 'What time is it in San Francisco?'}], provider='newvertex', deployment='gemini-1.5-pro-latest', timestamp=1723811876.5563881, parameters={'top_p': 1, 'top_k': 1, 'temperature': 1, 'max_output_tokens': 8192, 'frequency_penalty': 0, 'presence_penalty': 0}, metrics={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'cost_usd': 1.435e-05, 'latency_s': 1.6125669479370117, 'time_to_first_token_s': 1.5964012145996094, 'inter_token_latency_s': 0.0004382133483886719, 'tokens_per_second': 2.480517168677727})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You are a helpfull assistant\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"How can i help you?\"},\n",
    "           {\"role\": \"user\", \"content\": \"What time is it in San Francisco?\"}]\n",
    "\n",
    "# llm.chat('Hello', tools = functions)\n",
    "llm.chat(message, tools = functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = {'parts': [{'functionCall': {'name': 'get_time', 'args': {'location': 'San Francisco'}}}, {'functionCall': {'name': 'get_weather', 'args': {'location': 'San Francisco'}}}], 'role': 'model'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'id': '8ca9c165-31dc-4852-a4d8-d2d104ab1f04', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': 'assistant', 'tool_calls': [{'index': 0, 'id': 'call_testid1234', 'function': {'arguments': '', 'name': 'get_time', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723801960, 'model': 'gemini-1.0-pro', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'f5df5475-e923-435a-a060-ca19e18cd26f', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': \"{'location': 'San Francisco'}\", 'name': None}, 'type': None}]}, 'finish_reason': None, 'index': 0, 'logprobs': None}], 'created': 1723801960, 'model': 'gemini-1.0-pro', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "1\n",
      "{'id': 'f6536fc7-dd83-4941-9a83-2946ef5ed827', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': 'assistant', 'tool_calls': [{'index': 1, 'id': 'call_testid1234', 'function': {'arguments': '', 'name': 'get_weather', 'type': 'function'}, 'type': None}]}, 'finish_reason': None, 'index': 1, 'logprobs': None}], 'created': 1723801960, 'model': 'gemini-1.0-pro', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': 'af1505ba-ff65-4858-9f46-7c3d1d3d09d2', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': [{'index': 1, 'id': None, 'function': {'arguments': \"{'location': 'San Francisco'}\", 'name': None}, 'type': None}]}, 'finish_reason': None, 'index': 1, 'logprobs': None}], 'created': 1723801960, 'model': 'gemini-1.0-pro', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n",
      "{'id': '8bc8e2fb-d0cc-43db-84c3-96c03ab002f1', 'choices': [{'delta': {'content': None, 'function_call': None, 'role': None, 'tool_calls': None}, 'finish_reason': 'tool_calls', 'index': 0, 'logprobs': None}], 'created': 1723801960, 'model': 'gemini-1.0-pro', 'object': 'chat.completion.chunk', 'system_fingerprint': None, 'usage': None}\n"
     ]
    }
   ],
   "source": [
    "from openai.types.chat.chat_completion_chunk import (\n",
    "    Choice,\n",
    "    ChoiceDelta,\n",
    "    ChoiceDeltaFunctionCall,\n",
    "    ChoiceDeltaToolCall,\n",
    "    ChoiceDeltaToolCallFunction,\n",
    ")\n",
    "\n",
    "from openai.types.chat import ChatCompletionChunk\n",
    "\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "model = 'gemini-1.0-pro'\n",
    "\n",
    "if 'functionCall' in chunk['parts'][0]:\n",
    "    for index, functioncall in  enumerate(chunk['parts']):\n",
    "\n",
    "        first_chunk = ChatCompletionChunk(\n",
    "                            id=str(uuid.uuid4()),\n",
    "                            choices=[\n",
    "                                Choice(\n",
    "                                    delta=ChoiceDelta(\n",
    "                                        role=\"assistant\",\n",
    "                                        tool_calls=[\n",
    "                                            ChoiceDeltaToolCall(\n",
    "                                                index=index,\n",
    "                                                id=\"call_\" + \"testid1234\",\n",
    "                                                function=ChoiceDeltaToolCallFunction(\n",
    "                                                    name=functioncall['functionCall']['name'],\n",
    "                                                    arguments=\"\",\n",
    "                                                    type=\"function\",\n",
    "                                                ),\n",
    "                                            )\n",
    "                                        ],\n",
    "                                    ),\n",
    "                                    finish_reason=None,\n",
    "                                    index=index,\n",
    "                                )\n",
    "                            ],\n",
    "                            created=int(time.time()),\n",
    "                            model=model,\n",
    "                            object=\"chat.completion.chunk\",\n",
    "                        ).model_dump()\n",
    "        print(first_chunk)\n",
    "        \n",
    "        midle_chunk = ChatCompletionChunk(\n",
    "                            id=str(uuid.uuid4()),\n",
    "                            choices=[\n",
    "                                Choice(\n",
    "                                    delta=ChoiceDelta(\n",
    "                                        tool_calls=[\n",
    "                                            ChoiceDeltaToolCall(\n",
    "                                                index=index,\n",
    "                                                function=ChoiceDeltaToolCallFunction(\n",
    "                                                    arguments=str(functioncall['functionCall']['args']),\n",
    "                                                ),\n",
    "                                            )\n",
    "                                        ],\n",
    "                                    ),\n",
    "                                    finish_reason=None,\n",
    "                                    index=index,\n",
    "                                )\n",
    "                            ],\n",
    "                            created=int(time.time()),\n",
    "                            model=model,\n",
    "                            object=\"chat.completion.chunk\",\n",
    "                        ).model_dump()\n",
    "        print(midle_chunk)\n",
    "final_chunk = ChatCompletionChunk(\n",
    "                            id=str(uuid.uuid4()),\n",
    "                            choices=[\n",
    "                                Choice(\n",
    "                                    delta=ChoiceDelta(),\n",
    "                                    finish_reason=\"tool_calls\",\n",
    "                                    index=0,\n",
    "                                )\n",
    "                            ],\n",
    "                            created=int(time.time()),\n",
    "                            model=model,\n",
    "                            object=\"chat.completion.chunk\",\n",
    "                        ).model_dump()\n",
    "print(final_chunk)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Raw Vertex function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "  \"function_declarations\": [\n",
    "    {\n",
    "      \"name\": \"get_weather\",\n",
    "      \"description\": \"Gets the weather for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the weather for. For example: San Francisco\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"\n",
    "        ]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"get_time\",\n",
    "      \"description\": \"Gets the current time for a certain location.\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location we are getting the current time for. For example: New York\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\n",
    "          \"location\"  \n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m functions_json \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdumps(functions)\n\u001b[1;32m      2\u001b[0m functions_obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(functions_json)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mtype\u001b[39m(functions_obj)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "functions_json = json.dumps(functions)\n",
    "functions_obj = json.loads(functions_json)\n",
    "type(functions_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'functions_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m      7\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace this with your actual Google API key\u001b[39;00m\n\u001b[1;32m      8\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_instruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         }\n\u001b[1;32m     14\u001b[0m     },\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m         }\n\u001b[1;32m     20\u001b[0m     },\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfunctions_obj\u001b[49m,  \u001b[38;5;66;03m# Use the parsed object instead of the JSON string\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_calling_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTO\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     24\u001b[0m     },\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Make the API call\u001b[39;00m\n\u001b[1;32m     28\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m                          headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'functions_obj' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "# Prepare the data for the API call\n",
    "api_key = os.getenv('GOOGLE_API_KEY')  # Replace this with your actual Google API key\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\n",
    "    \"system_instruction\": {\n",
    "        \"parts\": {\n",
    "            \"text\": \"You are a helpful assistant.\"\n",
    "        }\n",
    "    },\n",
    "    \"contents\": {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": {\n",
    "            \"text\": \"Hello\"\n",
    "        }\n",
    "    },\n",
    "    \"tools\": functions_obj,  # Use the parsed object instead of the JSON string\n",
    "    \"tool_config\": {\n",
    "        \"function_calling_config\": {\"mode\": \"AUTO\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = requests.post(f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key={api_key}',\n",
    "                         headers=headers, json=data, stream=True)\n",
    "\n",
    "# Print the response content\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. OpenAI tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_time\",\n",
    "            \"description\": \"Get time of a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"system\", \"content\": \"You are a helpfull assistant\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"How can i help you?\"},\n",
    "           {\"role\": \"user\", \"content\": \"What is the weather in San Francisco? And what is the time?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "            model='gpt-4o',\n",
    "            messages=message,\n",
    "            tools=tools,\n",
    "            stream=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.Stream at 0x1204dd110>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_ASsylP9CsVhBLyyXGwIyY2fH', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"lo', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='catio', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='n\": \"S', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='an F', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='ranci', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='sco, C', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='A\", ', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"form', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='at\": \"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='fahr', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='enhei', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='t\"}', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id='call_7jpRAW2K5QZ066gpVaiYSwqY', function=ChoiceDeltaToolCallFunction(arguments='', name='get_time'), type='function')]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"lo', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='catio', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='n\": \"S', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='an F', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='ranci', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='sco, C', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='A\"}', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n",
      "ChatCompletionChunk(id='chatcmpl-9woLM1b1qGErhTbXA3UBQf2FhUAho', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None)], created=1723803572, model='gpt-4o-2024-05-13', object='chat.completion.chunk', system_fingerprint='fp_3aa7262c27', usage=None)\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VertexAI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmstudio import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLMstudio Tracking on http://localhost:50002 \n",
      "Running LLMstudio Engine on http://localhost:50001 \n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_departure(ticket_number: str):\n",
    "    \"\"\"Use this to fetch the departure time of a train\"\"\"\n",
    "    return \"12:00 AM\"\n",
    "\n",
    "@tool\n",
    "def cancel_ticket(ticket_number: str):\n",
    "    \"\"\"Use this to cancel a ticket\"\"\"\n",
    "    return \"Ticket cancelled\"\n",
    "\n",
    "@tool\n",
    "def buy_ticket(destination: str):\n",
    "    \"\"\"Use this to buy a ticket\"\"\"\n",
    "    return \"Bought ticket number 123456\"\n",
    "\n",
    "@tool\n",
    "def make_complaint(complaint: str):\n",
    "    \"\"\"Use this to forward a complaint to the complaint department\"\"\"\n",
    "    return \"Complaint forwarded. We appreciate your feedback.\"\n",
    "\n",
    "tools = [get_departure, cancel_ticket, buy_ticket, make_complaint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input'] optional_variables=['chat_history'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `buy_ticket` with `{'destination': 'Madrid'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mBought ticket number 123456\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_departure` with `{'ticket_number': '123456'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m12:00 AM\u001b[0m\u001b[32;1m\u001b[1;3mI have bought a ticket for you to Madrid. Your train will depart at 12:00 AM.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you buy me a ticket to madrid? At what time does my train deppart?',\n",
       " 'output': 'I have bought a ticket for you to Madrid. Your train will depart at 12:00 AM.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "from llmstudio.llm.langchain import ChatLLMstudio\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "model = ChatLLMstudio(model_id='vertex/gemini-1.5-flash', temperature=0)\n",
    "# model = ChatLLMstudio(model_id='openai/gpt-4o', temperature=0)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "agent = create_openai_tools_agent(llm=model, prompt=prompt, tools=tools)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"Can you buy me a ticket to madrid? At what time does my train deppart?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input'] optional_variables=['chat_history'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_openai_tools_agent(llm\u001b[38;5;241m=\u001b[39mmodel, prompt\u001b[38;5;241m=\u001b[39mprompt, tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[1;32m     14\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you buy me a ticket to madrid? At what time does my train deppart?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1432\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1432\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1441\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1442\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1138\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1148\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1148\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:1166\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain/agents/agent.py:514\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3254\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:2054\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 2054\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   2056\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:3211\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3209\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[0;32m-> 3211\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:1272\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m final: Input\n\u001b[1;32m   1270\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001b[39;49;00m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# then call stream.\u001b[39;49;00m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001b[39;49;00m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the `+` operator.\u001b[39;49;00m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# only operate on the last chunk,\u001b[39;49;00m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001b[39;49;00m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:5301\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   5296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5297\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   5298\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5299\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   5300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 5301\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   5302\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5304\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5305\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/runnables/base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:363\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[BaseMessageChunk]:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream(async_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}}):\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;66;03m# model doesn't implement streaming, so use default implementation\u001b[39;00m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(\n\u001b[0;32m--> 363\u001b[0m             BaseMessageChunk, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m         config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    750\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    754\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    755\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:613\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    612\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    614\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    615\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    617\u001b[0m ]\n\u001b[1;32m    618\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:603\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 603\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m         )\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llmstudiodev/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:829\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    826\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m         )\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# Add response metadata to each generation\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result\u001b[38;5;241m.\u001b[39mgenerations):\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/langchain.py:56\u001b[0m, in \u001b[0;36mChatLLMstudio._generate\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[BaseMessage], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m     55\u001b[0m     messages_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, [])\n\u001b[0;32m---> 56\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/GitHub/LLMstudio/llmstudio/llm/__init__.py:70\u001b[0m, in \u001b[0;36mLLM.chat\u001b[0;34m(self, input, is_stream, retries, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         error_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_data)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_stream:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_chat(response)\n",
      "\u001b[0;31mException\u001b[0m: Not Found"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "from llmstudio.llm.langchain import ChatLLMstudio\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# model = ChatLLMstudio(model_id='newvertex/gemini-1.5-pro-latest', temperature=0)\n",
    "# model = ChatLLMstudio(model_id='openai/gpt-4o', temperature=0)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "agent = create_openai_tools_agent(llm=model, prompt=prompt, tools=tools)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"Can you buy me a ticket to madrid? At what time does my train deppart?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmstudiodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
